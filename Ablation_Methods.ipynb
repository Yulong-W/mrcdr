{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ablation Methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yulong-W/mrcdr/blob/main/Ablation_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQGyOJ4lhI7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e41d61-a48f-4641-ff49-72e9b7bbac81"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdsWfUP0W85N"
      },
      "source": [
        "Ablation Method 1: Randomly shuffle the order of the sentences with explicit connectives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OutsAlqq6NU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation/Explicit Connectives.csv')\n",
        "\n",
        "Explicit_Connectives = data_frame['Explicit Connectives'].values.tolist()\n",
        "\n",
        "Explicit_Connectives = [' '.join(i.split('+')) if '+' in i else i for i in Explicit_Connectives]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FwHu9BI8rS5"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Random_Shuffle_Sentences_With_Explicit_Connectives(context):\n",
        "  Sentences_With_Explicit_Connectives = []\n",
        "\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens in each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "  \n",
        "  # obtain the r-length (r=1, 2, 3, 4) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length3_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 3)]\n",
        "    Length4_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 4)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length3_Subsets_of_Each_Sentence)):\n",
        "      k3 = ' '.join(Length3_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k3)\n",
        "    for k in range(len(Length4_Subsets_of_Each_Sentence)):\n",
        "      k4 = ' '.join(Length4_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k4)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "  \n",
        "  # test if a sentence contains the explicit connective\n",
        "  for i in range(len(Reference)):\n",
        "    intersec = [connectives for connectives in Reference[i] if connectives in Explicit_Connectives]\n",
        "    intersec = list(set(intersec))\n",
        "    if len(intersec) > 0:\n",
        "      Sentences_With_Explicit_Connectives.append(Sentences[i])\n",
        "\n",
        "  Sentences_Not_With_EC = [sentence for sentence in Sentences if sentence not in Sentences_With_Explicit_Connectives]\n",
        "\n",
        "  # randomly shuffle the order of the sentences with explicit connectives\n",
        "  import random\n",
        "  random.shuffle(Sentences_With_Explicit_Connectives)\n",
        "  Reshuffled_Context = Sentences_Not_With_EC + Sentences_With_Explicit_Connectives\n",
        "  Reshuffled_Context.append('')\n",
        "\n",
        "  context = '.'.join(Reshuffled_Context)\n",
        "  return context"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFZRoGBCXfC9"
      },
      "source": [
        "Ablation Method 2: Mask explicit connectives associated with sense 2 (Temporal.Asynchronous.Precedence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y25RmR0SwxEI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation/Temporal.Asynchronous.Precedence.csv')\n",
        "\n",
        "Sense_2 = data_frame['Temporal.Asynchronous.Precedence'].values.tolist()\n",
        "\n",
        "Sense_2_Single = [i for i in Sense_2 if len(i.split()) == 1]\n",
        "\n",
        "Sense_2_Multiple = [i for i in Sense_2 if i not in Sense_2_Single]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "651eqUGDsWDX"
      },
      "source": [
        "def Neighbor_Two(L, s1, s2):\n",
        "  indices = [i for i, x in enumerate(L) if x == s1]\n",
        "  indices1 = [i for i, x in enumerate(L) if x == s2]\n",
        "  position_list = []\n",
        "  value_list = []\n",
        "  for i in range(len(indices1)):\n",
        "    for j in range(len(indices)):\n",
        "      position_list.append(str(indices[j])+' '+str(indices1[i]))\n",
        "      value_list.append(indices1[i]-indices[j])\n",
        "  Dic = dict(zip(value_list, position_list))\n",
        "  return Dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2bl2zlJDMb_"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_2(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "  \n",
        "  # obtain the r-length (r=1 or 2) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "  \n",
        "  # test if the sentence contains the explicit connectives associated with sense 2\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_2_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple = [connectives for connectives in Reference[i] if connectives in Sense_2_Multiple]\n",
        "    Intersec_Multiple = list(set(Intersec_Multiple))\n",
        "    \n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple) > 0:\n",
        "      w = Intersec_Multiple[0].split()\n",
        "      Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0], w[1])\n",
        "\n",
        "      keys_list = []\n",
        "      for key in Dic.keys():\n",
        "        keys_list.append(key)\n",
        "\n",
        "      if 1 in keys_list:\n",
        "        position = Dic[1]\n",
        "        position_list = position.split()\n",
        "\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Lower[int(position_list[0])] = 'UNK'\n",
        "        Lower[int(position_list[1])] = 'UNK'\n",
        "        Sentences[i] = ' '.join(Lower)\n",
        "  \n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fli7yW6BDqz4"
      },
      "source": [
        "Mask explicit connectives associated with sense 2 (Temporal.Asynchronous.Precedence) - Full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6GbvTtFDwnN"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation-Full/Temporal.Asynchronous.Precedence.csv')\n",
        "\n",
        "Sense_2 = data_frame['Temporal.Asynchronous.Precedence'].values.tolist()\n",
        "\n",
        "Sense_2_Single = [i for i in Sense_2 if len(i.split()) == 1]\n",
        "\n",
        "Sense_2_Multiple_Two = [i for i in Sense_2 if len(i.split()) == 2]\n",
        "\n",
        "Sense_2_Multiple_Three = [i for i in Sense_2 if len(i.split()) == 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmuWrPUbFyw9"
      },
      "source": [
        "def Neighbor_Two(L, s1, s2):\n",
        "  indices = [i for i, x in enumerate(L) if x == s1]\n",
        "  indices1 = [i for i, x in enumerate(L) if x == s2]\n",
        "  position_list = []\n",
        "  value_list = []\n",
        "  for i in range(len(indices1)):\n",
        "    for j in range(len(indices)):\n",
        "      position_list.append(str(indices[j])+' '+str(indices1[i]))\n",
        "      value_list.append(indices1[i]-indices[j])\n",
        "  Dic = dict(zip(value_list, position_list))\n",
        "  return Dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LPSF9xNF4UZ"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_2_Full(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "  \n",
        "  # obtain the r-length (r=1 or 2 or 3) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length3_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 3)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length3_Subsets_of_Each_Sentence)):\n",
        "      k3 = ' '.join(Length3_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k3)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "  \n",
        "  # test if the sentence contains the explicit connectives associated with sense 2\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_2_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_2_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Three = [connectives for connectives in Reference[i] if connectives in Sense_2_Multiple_Three]\n",
        "    Intersec_Multiple_Three = list(set(Intersec_Multiple_Three))\n",
        "    \n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = Intersec_Multiple_Two[0].split()\n",
        "      Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0], w[1])\n",
        "\n",
        "      keys_list = []\n",
        "      for key in Dic.keys():\n",
        "        keys_list.append(key)\n",
        "\n",
        "      if 1 in keys_list:\n",
        "        position = Dic[1]\n",
        "        position_list = position.split()\n",
        "\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Lower[int(position_list[0])] = 'UNK'\n",
        "        Lower[int(position_list[1])] = 'UNK'\n",
        "        Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Multiple_Three) > 0:\n",
        "      w = Intersec_Multiple_Three[0].split()\n",
        "\n",
        "      Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0], w[2])\n",
        "\n",
        "      keys_list = []\n",
        "      for key in Dic.keys():\n",
        "        keys_list.append(key)\n",
        "\n",
        "      if 2 in keys_list:\n",
        "        position = Dic[2]\n",
        "        position_list = position.split()\n",
        "\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Lower[int(position_list[0])] = 'UNK'\n",
        "        Lower[int(position_list[0])+1] = 'UNK'\n",
        "        Lower[int(position_list[1])] = 'UNK'\n",
        "        Sentences[i] = ' '.join(Lower)\n",
        "  \n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1p6JdpvXuS3"
      },
      "source": [
        "Ablation Method 3: Mask explicit connectives associated with sense 3 (Temporal.Asynchronous.Succession)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q7zPAJJDQnx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation/Temporal.Asynchronous.Succession.csv')\n",
        "\n",
        "Sense_3 = data_frame['Temporal.Asynchronous.Succession'].values.tolist()\n",
        "\n",
        "Sense_3_Single = [i for i in Sense_3 if len(i.split()) == 1]\n",
        "\n",
        "Sense_3_Multiple = [i for i in Sense_3 if i not in Sense_3_Single]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfnU8tFOFylf"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_3(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "  \n",
        "  # obtain the r-length (r=1 or 2) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "  \n",
        "  # test if the sentence contains the explicit connectives associated with sense 3\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_3_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple = [connectives for connectives in Reference[i] if connectives in Sense_3_Multiple]\n",
        "    Intersec_Multiple = list(set(Intersec_Multiple))\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple) > 0:\n",
        "      w = [Intersec_Multiple[j].split() for j in range(len(Intersec_Multiple))]\n",
        "      if len(Intersec_Multiple) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxi94qEBLjav"
      },
      "source": [
        "Mask explicit connectives associated with sense 3 (Temporal.Asynchronous.Succession) - Full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TJM0BBdLsRA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation-Full/Temporal.Asynchronous.Succession.csv')\n",
        "\n",
        "Sense_3 = data_frame['Temporal.Asynchronous.Succession'].values.tolist()\n",
        "\n",
        "Sense_3_Single = [i for i in Sense_3 if len(i.split()) == 1]\n",
        "\n",
        "Sense_3_Multiple_Two = [i for i in Sense_3 if len(i.split()) == 2]\n",
        "\n",
        "Sense_3_Multiple_Three = [i for i in Sense_3 if len(i.split()) == 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVO--Ow8NgsW"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_3_Full(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "  \n",
        "  # obtain the r-length (r=1 or 2 or 3) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length3_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 3)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length3_Subsets_of_Each_Sentence)):\n",
        "      k3 = ' '.join(Length3_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k3)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "  \n",
        "  # test if the sentence contains the explicit connectives associated with sense 3\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_3_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_3_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Three = [connectives for connectives in Reference[i] if connectives in Sense_3_Multiple_Three]\n",
        "    Intersec_Multiple_Three = list(set(Intersec_Multiple_Three))\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = [Intersec_Multiple_Two[j].split() for j in range(len(Intersec_Multiple_Two))]\n",
        "      if len(Intersec_Multiple_Two) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "    \n",
        "    if len(Intersec_Multiple_Three) > 0:\n",
        "      w = [Intersec_Multiple_Three[j].split() for j in range(len(Intersec_Multiple_Three))]\n",
        "      if len(Intersec_Multiple_Three) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B079cg9sX7wu"
      },
      "source": [
        "Ablation Method 4: Mask explicit connectives associated with sense 4 (Contingency.Cause.Reason)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQG8Fd3_i1vp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation/Contingency.Cause.Reason.csv')\n",
        "\n",
        "Sense_4 = data_frame['Contingency.Cause.Reason'].values.tolist()\n",
        "\n",
        "Sense_4_Single = [i for i in Sense_4 if len(i.split()) == 1]\n",
        "\n",
        "Sense_4_Multiple_Two = [i for i in Sense_4 if len(i.split()) == 2]\n",
        "\n",
        "Sense_4_Multiple_Four = [i for i in Sense_4 if len(i.split()) == 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MP4xwKllqTe"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_4(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "  \n",
        "  # obtain the r-length (r=1 or 2 or 4) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length4_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 4)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length4_Subsets_of_Each_Sentence)):\n",
        "      k4 = ' '.join(Length4_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k4)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "  \n",
        "  # test if the sentence contains the explicit connectives associated with sense 4\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_4_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_4_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Four = [connectives for connectives in Reference[i] if connectives in Sense_4_Multiple_Four]\n",
        "    Intersec_Multiple_Four = list(set(Intersec_Multiple_Four))\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = [Intersec_Multiple_Two[j].split() for j in range(len(Intersec_Multiple_Two))]\n",
        "      if len(Intersec_Multiple_Two) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Multiple_Four) > 0:\n",
        "      w = Intersec_Multiple_Four[0].split()\n",
        "\n",
        "      Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0], w[3])\n",
        "\n",
        "      keys_list = []\n",
        "      for key in Dic.keys():\n",
        "        keys_list.append(key)\n",
        "\n",
        "      if 3 in keys_list:\n",
        "        position = Dic[3]\n",
        "        position_list = position.split()\n",
        "\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Lower[int(position_list[0])] = 'UNK'\n",
        "        Lower[int(position_list[0])+1] = 'UNK'\n",
        "        Lower[int(position_list[0])+2] = 'UNK'\n",
        "        Lower[int(position_list[1])] = 'UNK'\n",
        "        Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JoVVpGPr6W"
      },
      "source": [
        "Mask explicit connectives associated with sense 4 (Contingency.Cause.Reason) - Full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_biBdRJ-Puh9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation-Further/Contingency.Cause.Reason.csv')\n",
        "\n",
        "Sense_4 = data_frame['Contingency.Cause.Reason'].values.tolist()\n",
        "\n",
        "Sense_4_Single = [i for i in Sense_4 if len(i.split()) == 1]\n",
        "\n",
        "Sense_4_Multiple_Two = [i for i in Sense_4 if len(i.split()) == 2]\n",
        "\n",
        "Sense_4_Multiple_Four = [i for i in Sense_4 if len(i.split()) == 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzmQicWZQKng"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_4_Full(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "\n",
        "  # obtain the r-length (r=1 or 2 or 4) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length4_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 4)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length4_Subsets_of_Each_Sentence)):\n",
        "      k4 = ' '.join(Length4_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k4)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "  \n",
        "  # test if the sentence contains the explicit connectives associated with sense 4\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_4_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_4_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Four = [connectives for connectives in Reference[i] if connectives in Sense_4_Multiple_Four]\n",
        "    Intersec_Multiple_Four = list(set(Intersec_Multiple_Four))\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = [Intersec_Multiple_Two[j].split() for j in range(len(Intersec_Multiple_Two))]\n",
        "      if len(Intersec_Multiple_Two) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 2:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 3:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Multiple_Four) > 0:\n",
        "      w = Intersec_Multiple_Four[0].split()\n",
        "\n",
        "      Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0], w[3])\n",
        "\n",
        "      keys_list = []\n",
        "      for key in Dic.keys():\n",
        "        keys_list.append(key)\n",
        "\n",
        "      if 3 in keys_list:\n",
        "        position = Dic[3]\n",
        "        position_list = position.split()\n",
        "\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Lower[int(position_list[0])] = 'UNK'\n",
        "        Lower[int(position_list[0])+1] = 'UNK'\n",
        "        Lower[int(position_list[0])+2] = 'UNK'\n",
        "        Lower[int(position_list[1])] = 'UNK'\n",
        "        Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIqEkwTRYJZi"
      },
      "source": [
        "Ablation Method 5: Mask explicit connectives associated with sense 12 (Contingency.Condition.Arg2-as-cond)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSo9T1doEmDX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation/Contingency.Condition.Arg2-as-cond.csv')\n",
        "\n",
        "Sense_12 = data_frame['Contingency.Condition.Arg2-as-cond'].values.tolist()\n",
        "\n",
        "Sense_12_Single = [i for i in Sense_12 if len(i.split()) == 1 and '+' not in i]\n",
        "\n",
        "Sense_12_Discontinuous = [i for i in Sense_12 if len(i.split()) == 1 and '+' in i]\n",
        "Sense_12_Discontinuous = [' '.join(Sense_12_Discontinuous[0].split('+'))]\n",
        "\n",
        "Sense_12_Multiple_Two = [i for i in Sense_12 if len(i.split()) == 2]\n",
        "\n",
        "Sense_12_Multiple_Three = [i for i in Sense_12 if len(i.split()) == 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD0aGLLNjYlv"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_12(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "  \n",
        "  # obtain the r-length (r=1 or 2 or 3) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length3_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 3)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length3_Subsets_of_Each_Sentence)):\n",
        "      k3 = ' '.join(Length3_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k3)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "  \n",
        "  # test if the sentence contains the explicit connectives associated with sense 12\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_12_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_12_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Three = [connectives for connectives in Reference[i] if connectives in Sense_12_Multiple_Three]\n",
        "    Intersec_Multiple_Three = list(set(Intersec_Multiple_Three))\n",
        "    \n",
        "    Intersec_Discontinuous = [connectives for connectives in Reference[i] if connectives in Sense_12_Discontinuous]\n",
        "    Intersec_Discontinuous = list(set(Intersec_Discontinuous))\n",
        "\n",
        "    if len(Intersec_Discontinuous) > 0:\n",
        "      w = Intersec_Discontinuous[0].split()\n",
        "\n",
        "      position1 = Processed_Tokens_In_Each_Sentence[i].index(w[0])\n",
        "      position2 = Processed_Tokens_In_Each_Sentence[i].index(w[1])\n",
        "      w1 = []\n",
        "      w1.append(w[0])\n",
        "      w1.append(w[1])\n",
        "      if position2-position1 > 1:\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple_Three) > 0:\n",
        "      w = Intersec_Multiple_Three[0].split()\n",
        "\n",
        "      Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0], w[2])\n",
        "\n",
        "      keys_list = []\n",
        "      for key in Dic.keys():\n",
        "        keys_list.append(key)\n",
        "\n",
        "      if 2 in keys_list:\n",
        "        position = Dic[2]\n",
        "        position_list = position.split()\n",
        "\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Lower[int(position_list[0])] = 'UNK'\n",
        "        Lower[int(position_list[0])+1] = 'UNK'\n",
        "        Lower[int(position_list[1])] = 'UNK'\n",
        "        Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = [Intersec_Multiple_Two[j].split() for j in range(len(Intersec_Multiple_Two))]\n",
        "      if len(Intersec_Multiple_Two) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 2:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 3:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WonvTWj4Rpyh"
      },
      "source": [
        "Mask explicit connectives associated with sense 12 (Contingency.Condition.Arg2-as-cond) - Full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQPnzwoqRsKB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation-Further/Contingency.Condition.Arg2-as-cond.csv')\n",
        "\n",
        "Sense_12 = data_frame['Contingency.Condition.Arg2-as-cond'].values.tolist()\n",
        "\n",
        "Sense_12_Single = [i for i in Sense_12 if len(i.split()) == 1 and '+' not in i]\n",
        "\n",
        "Sense_12_Discontinuous = [i for i in Sense_12 if len(i.split()) == 1 and '+' in i]\n",
        "Sense_12_Discontinuous = [' '.join(Sense_12_Discontinuous[0].split('+'))]\n",
        "\n",
        "Sense_12_Multiple_Two = [i for i in Sense_12 if len(i.split()) == 2]\n",
        "\n",
        "Sense_12_Multiple_Three = [i for i in Sense_12 if len(i.split()) == 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eT5uVf4SbLe"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_12_Full(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        " \n",
        "  # obtain the r-length (r=1 or 2 or 3) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length3_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 3)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length3_Subsets_of_Each_Sentence)):\n",
        "      k3 = ' '.join(Length3_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k3)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "\n",
        "  # test if the sentence contains the explicit connectives associated with sense 12\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_12_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_12_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Three = [connectives for connectives in Reference[i] if connectives in Sense_12_Multiple_Three]\n",
        "    Intersec_Multiple_Three = list(set(Intersec_Multiple_Three))\n",
        "    \n",
        "    Intersec_Discontinuous = [connectives for connectives in Reference[i] if connectives in Sense_12_Discontinuous]\n",
        "    Intersec_Discontinuous = list(set(Intersec_Discontinuous))\n",
        "\n",
        "    if len(Intersec_Discontinuous) > 0:\n",
        "      w = Intersec_Discontinuous[0].split()\n",
        "\n",
        "      position1 = Processed_Tokens_In_Each_Sentence[i].index(w[0])\n",
        "      position2 = Processed_Tokens_In_Each_Sentence[i].index(w[1])\n",
        "      w1 = []\n",
        "      w1.append(w[0])\n",
        "      w1.append(w[1])\n",
        "      if position2-position1 > 1:\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple_Three) > 0:\n",
        "      w = [Intersec_Multiple_Three[j].split() for j in range(len(Intersec_Multiple_Three))]\n",
        "      if len(Intersec_Multiple_Three) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Three) == 2:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = [Intersec_Multiple_Two[j].split() for j in range(len(Intersec_Multiple_Two))]\n",
        "      if len(Intersec_Multiple_Two) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 2:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 3:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 4:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 5:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[4][0], w[4][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[4][0], w[4][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[5][0], w[5][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27ey4MoqYSgJ"
      },
      "source": [
        "Ablation Method 6: Mask explicit connectives associated with sense 20 (Comparison.Concession.Arg2-as-denier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9xQy-GkRMDD"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation/Comparison.Concession.Arg2-as-denier.csv')\n",
        "\n",
        "Sense_20 = data_frame['Comparison.Concession.Arg2-as-denier'].values.tolist()\n",
        "\n",
        "Sense_20_Single = [i for i in Sense_20 if len(i.split()) == 1]\n",
        "\n",
        "Sense_20_Multiple_Two = [i for i in Sense_20 if len(i.split()) == 2]\n",
        "\n",
        "Sense_20_Multiple_Three = [i for i in Sense_20 if len(i.split()) == 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYj7GThQjnQ_"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_20(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        " \n",
        "  # obtain the r-length (r=1 or 2 or 3) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length3_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 3)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length3_Subsets_of_Each_Sentence)):\n",
        "      k3 = ' '.join(Length3_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k3)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "\n",
        "  # test if the sentence contains the explicit connectives associated with sense 20\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_20_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_20_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Three = [connectives for connectives in Reference[i] if connectives in Sense_20_Multiple_Three]\n",
        "    Intersec_Multiple_Three = list(set(Intersec_Multiple_Three))\n",
        "\n",
        "    if len(Intersec_Multiple_Three) > 0:\n",
        "      w = [Intersec_Multiple_Three[j].split() for j in range(len(Intersec_Multiple_Three))]\n",
        "      if len(Intersec_Multiple_Three) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = [Intersec_Multiple_Two[j].split() for j in range(len(Intersec_Multiple_Two))]\n",
        "      if len(Intersec_Multiple_Two) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxBYB5DFWwKT"
      },
      "source": [
        "Mask explicit connectives associated with sense 20 (Comparison.Concession.Arg2-as-denier) - Full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dxuvQp9W00v"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation-Full/Comparison.Concession.Arg2-as-denier.csv')\n",
        "\n",
        "Sense_20 = data_frame['Comparison.Concession.Arg2-as-denier'].values.tolist()\n",
        "\n",
        "Sense_20_Single = [i for i in Sense_20 if len(i.split()) == 1]\n",
        "\n",
        "Sense_20_Multiple_Two = [i for i in Sense_20 if len(i.split()) == 2 and '+' not in i]\n",
        "\n",
        "Sense_20_Multiple_Three = [i for i in Sense_20 if len(i.split()) == 3]\n",
        "\n",
        "Sense_20_Multiple_Four = [i for i in Sense_20 if len(i.split()) == 4]\n",
        "\n",
        "Sense_20_Discontinuous = [i for i in Sense_20 if '+' in i]\n",
        "Sense_20_Discontinuous = [' '.join(Sense_20_Discontinuous[j].split('+')) for j in range(len(Sense_20_Discontinuous))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pep3fSUzYBar"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_20_Full(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "\n",
        "  # obtain the r-length (r=1 or 2 or 3 or 4) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length3_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 3)]\n",
        "    Length4_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 4)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length3_Subsets_of_Each_Sentence)):\n",
        "      k3 = ' '.join(Length3_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k3)\n",
        "    for k in range(len(Length4_Subsets_of_Each_Sentence)):\n",
        "      k4 = ' '.join(Length4_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k4)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "\n",
        "  # test if the sentence contains the explicit connectives associated with sense 20\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_20_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_20_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Three = [connectives for connectives in Reference[i] if connectives in Sense_20_Multiple_Three]\n",
        "    Intersec_Multiple_Three = list(set(Intersec_Multiple_Three))\n",
        "    \n",
        "    Intersec_Multiple_Four = [connectives for connectives in Reference[i] if connectives in Sense_20_Multiple_Four]\n",
        "    Intersec_Multiple_Four = list(set(Intersec_Multiple_Four))\n",
        "    \n",
        "    Intersec_Discontinuous = [connectives for connectives in Reference[i] if connectives in Sense_20_Discontinuous]\n",
        "    Intersec_Discontinuous = list(set(Intersec_Discontinuous))\n",
        "\n",
        "    if len(Intersec_Multiple_Three) > 0:\n",
        "      w = [Intersec_Multiple_Three[j].split() for j in range(len(Intersec_Multiple_Three))]\n",
        "      if len(Intersec_Multiple_Three) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Three) == 2:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = [Intersec_Multiple_Two[j].split() for j in range(len(Intersec_Multiple_Two))]\n",
        "      if len(Intersec_Multiple_Two) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 2:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 3:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 4:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 5:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[4][0], w[4][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[4][0], w[4][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[5][0], w[5][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple_Four) > 0:\n",
        "      w = Intersec_Multiple_Four[0].split()\n",
        "\n",
        "      Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0], w[3])\n",
        "\n",
        "      keys_list = []\n",
        "      for key in Dic.keys():\n",
        "        keys_list.append(key)\n",
        "\n",
        "      if 3 in keys_list:\n",
        "        position = Dic[3]\n",
        "        position_list = position.split()\n",
        "\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Lower[int(position_list[0])] = 'UNK'\n",
        "        Lower[int(position_list[0])+1] = 'UNK'\n",
        "        Lower[int(position_list[0])+2] = 'UNK'\n",
        "        Lower[int(position_list[1])] = 'UNK'\n",
        "        Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Discontinuous) > 0:\n",
        "      w = [Intersec_Discontinuous[j].split() for j in range(len(Intersec_Discontinuous))]\n",
        "      if len(Intersec_Discontinuous) == 1:\n",
        "        if len(w[0]) == 3:\n",
        "          position1 = Processed_Tokens_In_Each_Sentence[i].index(w[0][0])\n",
        "          position2 = Processed_Tokens_In_Each_Sentence[i].index(w[0][1])\n",
        "          position3 = Processed_Tokens_In_Each_Sentence[i].index(w[0][2])\n",
        "          w1 = []\n",
        "          w1.append(w[0][0])\n",
        "          w1.append(w[0][1])\n",
        "          w1.append(w[0][2])\n",
        "          if position2-position1 == 1 and position3-position2 > 1:\n",
        "            Lower = [token.lower() for token in Sentences[i].split()]\n",
        "            Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRoGbBpqYbeu"
      },
      "source": [
        "Ablation Method 7: Mask explicit connectives associated with sense 24 (Expansion.Conjunction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4uh4hNwm9cf"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation/Expansion.Conjunction.csv')\n",
        "\n",
        "Sense_24 = data_frame['Expansion.Conjunction'].values.tolist()\n",
        "\n",
        "Sense_24_Continuous = [i for i in Sense_24 if '+' not in i]\n",
        "\n",
        "Sense_24_Single = [i for i in Sense_24_Continuous if len(i.split()) == 1]\n",
        "\n",
        "Sense_24_Multiple_Two = [i for i in Sense_24_Continuous if len(i.split()) == 2]\n",
        "\n",
        "Sense_24_Multiple_Three = [i for i in Sense_24_Continuous if len(i.split()) == 3]\n",
        "\n",
        "Sense_24_Discontinuous = [i for i in Sense_24 if '+' in i]\n",
        "Sense_24_Discontinuous = [' '.join(Sense_24_Discontinuous[j].split('+')) for j in range(len(Sense_24_Discontinuous))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVH96sYNu1qr"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_24(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "  \n",
        "  # obtain the r-length (r=1 or 2 or 3 or 4) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length3_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 3)]\n",
        "    Length4_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 4)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length3_Subsets_of_Each_Sentence)):\n",
        "      k3 = ' '.join(Length3_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k3)\n",
        "    for k in range(len(Length4_Subsets_of_Each_Sentence)):\n",
        "      k4 = ' '.join(Length4_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k4)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        " \n",
        "  # test if the sentence contains the explicit connectives associated with sense 24\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_24_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_24_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Three = [connectives for connectives in Reference[i] if connectives in Sense_24_Multiple_Three]\n",
        "    Intersec_Multiple_Three = list(set(Intersec_Multiple_Three))\n",
        "    \n",
        "    Intersec_Discontinuous = [connectives for connectives in Reference[i] if connectives in Sense_24_Discontinuous]\n",
        "    Intersec_Discontinuous = list(set(Intersec_Discontinuous))\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple_Three) > 0:\n",
        "      w = Intersec_Multiple_Three[0].split()\n",
        "      Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0], w[2])\n",
        "\n",
        "      keys_list = []\n",
        "      for key in Dic.keys():\n",
        "        keys_list.append(key)\n",
        "\n",
        "      if 2 in keys_list:\n",
        "        position = Dic[2]\n",
        "        position_list = position.split()\n",
        "\n",
        "        Lower = [token.lower() for token in Sentences[i].split()]\n",
        "        Lower[int(position_list[0])] = 'UNK'\n",
        "        Lower[int(position_list[0])+1] = 'UNK'\n",
        "        Lower[int(position_list[1])] = 'UNK'\n",
        "        Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = [Intersec_Multiple_Two[j].split() for j in range(len(Intersec_Multiple_Two))]\n",
        "      if len(Intersec_Multiple_Two) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 2:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 3:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 4:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[4][0], w[4][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Discontinuous) > 0:\n",
        "      w = [Intersec_Discontinuous[j].split() for j in range(len(Intersec_Discontinuous))]\n",
        "      if len(Intersec_Discontinuous) == 1:\n",
        "        if len(w[0]) == 2:\n",
        "          position1 = Processed_Tokens_In_Each_Sentence[i].index(w[0][0])\n",
        "          position2 = Processed_Tokens_In_Each_Sentence[i].index(w[0][1])\n",
        "          w1 = []\n",
        "          w1.append(w[0][0])\n",
        "          w1.append(w[0][1])\n",
        "          if position2-position1 > 1:\n",
        "            Lower = [token.lower() for token in Sentences[i].split()]\n",
        "            Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "        elif len(w[0]) == 3:\n",
        "          position1 = Processed_Tokens_In_Each_Sentence[i].index(w[0][0])\n",
        "          position2 = Processed_Tokens_In_Each_Sentence[i].index(w[0][1])\n",
        "          position3 = Processed_Tokens_In_Each_Sentence[i].index(w[0][2])\n",
        "          w1 = []\n",
        "          w1.append(w[0][0])\n",
        "          w1.append(w[0][1])\n",
        "          w1.append(w[0][2])\n",
        "          if position2-position1 == 1 and position3-position2 > 1:\n",
        "            Lower = [token.lower() for token in Sentences[i].split()]\n",
        "            Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "        else:\n",
        "          position1 = Processed_Tokens_In_Each_Sentence[i].index(w[0][0])\n",
        "          position2 = Processed_Tokens_In_Each_Sentence[i].index(w[0][1])\n",
        "          position3 = Processed_Tokens_In_Each_Sentence[i].index(w[0][2])\n",
        "          position4 = Processed_Tokens_In_Each_Sentence[i].index(w[0][3])\n",
        "          w1 = []\n",
        "          w1.append(w[0][0])\n",
        "          w1.append(w[0][1])\n",
        "          w1.append(w[0][2])\n",
        "          w1.append(w[0][3])\n",
        "          if position2-position1 == 1 and position3-position2 > 1 and position4-position3 > 1:\n",
        "            Lower = [token.lower() for token in Sentences[i].split()]\n",
        "            Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "          if position2-position1 == 1 and position3-position2 > 1 and position4-position3 == 1:\n",
        "            Lower = [token.lower() for token in Sentences[i].split()]\n",
        "            Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rT7v_vvcQRD"
      },
      "source": [
        "Mask explicit connectives associated with sense 24 (Expansion.Conjunction) - Full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwKSJy_BcSIq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/Evaluation-Full/Expansion.Conjunction.csv')\n",
        "\n",
        "Sense_24 = data_frame['Expansion.Conjunction'].values.tolist()\n",
        "\n",
        "Sense_24_Continuous = [i for i in Sense_24 if '+' not in i]\n",
        "\n",
        "Sense_24_Single = [i for i in Sense_24_Continuous if len(i.split()) == 1]\n",
        "\n",
        "Sense_24_Multiple_Two = [i for i in Sense_24_Continuous if len(i.split()) == 2]\n",
        "\n",
        "Sense_24_Multiple_Three = [i for i in Sense_24_Continuous if len(i.split()) == 3]\n",
        "\n",
        "Sense_24_Discontinuous = [i for i in Sense_24 if '+' in i]\n",
        "Sense_24_Discontinuous = [' '.join(Sense_24_Discontinuous[j].split('+')) for j in range(len(Sense_24_Discontinuous))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uF362g3c9fV"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def rSubset(arr, r):\n",
        "  return list(combinations(arr, r))\n",
        "\n",
        "def Mask_Explicit_Connectives_With_Sense_24_Full(context):\n",
        "  # sentence segmentation\n",
        "  Sentences = context.split('.')\n",
        "  Sentences = [sentence for sentence in Sentences if len(sentence.split()) > 0]\n",
        "\n",
        "  # tokenization\n",
        "  Tokens_In_Each_Sentence = [sentence.split() for sentence in Sentences]\n",
        "\n",
        "  # delete the punctuations contained in the tokens of each sentence\n",
        "  # lower the tokens\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  Processed_Tokens_In_Each_Sentence = []\n",
        "  for i in range(len(Tokens_In_Each_Sentence)):\n",
        "    Processed_Tokens = [w.translate(table) for w in Tokens_In_Each_Sentence[i]]\n",
        "    Lowercase_Tokens = [w.lower() for w in Processed_Tokens]\n",
        "    Processed_Tokens_In_Each_Sentence.append(Lowercase_Tokens)\n",
        "  \n",
        "  # obtain the r-length (r=1 or 2 or 3 or 4) subsets of each list in the \"Processed_Tokens_In_Each_Sentence\"\n",
        "  Reference = []\n",
        "  for j in range(len(Processed_Tokens_In_Each_Sentence)):\n",
        "    Length1_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 1)]\n",
        "    Length2_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 2)]\n",
        "    Length3_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 3)]\n",
        "    Length4_Subsets_of_Each_Sentence = [x for x in rSubset(Processed_Tokens_In_Each_Sentence[j], 4)]\n",
        "\n",
        "    Processed_Subsets_of_Each_Sentence = []\n",
        "    for k in range(len(Length1_Subsets_of_Each_Sentence)):\n",
        "      k1 = ' '.join(Length1_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k1)\n",
        "    for k in range(len(Length2_Subsets_of_Each_Sentence)):\n",
        "      k2 = ' '.join(Length2_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k2)\n",
        "    for k in range(len(Length3_Subsets_of_Each_Sentence)):\n",
        "      k3 = ' '.join(Length3_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k3)\n",
        "    for k in range(len(Length4_Subsets_of_Each_Sentence)):\n",
        "      k4 = ' '.join(Length4_Subsets_of_Each_Sentence[k])\n",
        "      Processed_Subsets_of_Each_Sentence.append(k4)\n",
        "    \n",
        "    Reference.append(Processed_Subsets_of_Each_Sentence)\n",
        "  \n",
        "  # test if the sentence contains the explicit connectives associated with sense 24\n",
        "  for i in range(len(Reference)):\n",
        "    Intersec_Single = [connectives for connectives in Reference[i] if connectives in Sense_24_Single]\n",
        "    Intersec_Single = list(set(Intersec_Single))\n",
        "    \n",
        "    Intersec_Multiple_Two = [connectives for connectives in Reference[i] if connectives in Sense_24_Multiple_Two]\n",
        "    Intersec_Multiple_Two = list(set(Intersec_Multiple_Two))\n",
        "    \n",
        "    Intersec_Multiple_Three = [connectives for connectives in Reference[i] if connectives in Sense_24_Multiple_Three]\n",
        "    Intersec_Multiple_Three = list(set(Intersec_Multiple_Three))\n",
        "    \n",
        "    Intersec_Discontinuous = [connectives for connectives in Reference[i] if connectives in Sense_24_Discontinuous]\n",
        "    Intersec_Discontinuous = list(set(Intersec_Discontinuous))\n",
        "\n",
        "    if len(Intersec_Single) > 0:\n",
        "      w = [Intersec_Single[j].split() for j in range(len(Intersec_Single))]\n",
        "      processed_w = [w[j][0] for j in range(len(Intersec_Single))]\n",
        "      Lower = [token.lower() for token in Sentences[i].split()]\n",
        "      Sentences[i] = ' '.join(['UNK' if token in processed_w else token for token in Lower])\n",
        "\n",
        "    if len(Intersec_Multiple_Three) > 0:\n",
        "      w = [Intersec_Multiple_Three[j].split() for j in range(len(Intersec_Multiple_Three))]\n",
        "      if len(Intersec_Multiple_Three) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Three) == 2:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][2])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 2 in keys_list:\n",
        "          position = Dic[2]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[0])+1] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Multiple_Two) > 0:\n",
        "      w = [Intersec_Multiple_Two[j].split() for j in range(len(Intersec_Multiple_Two))]\n",
        "      if len(Intersec_Multiple_Two) == 1:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 2:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 3:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 4:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 5:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[4][0], w[4][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      elif len(Intersec_Multiple_Two) == 6:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[4][0], w[4][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[5][0], w[5][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "      else:\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[0][0], w[0][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[1][0], w[1][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[2][0], w[2][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[3][0], w[3][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[4][0], w[4][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[5][0], w[5][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "        \n",
        "        Dic = Neighbor_Two(Processed_Tokens_In_Each_Sentence[i], w[6][0], w[6][1])\n",
        "\n",
        "        keys_list = []\n",
        "        for key in Dic.keys():\n",
        "          keys_list.append(key)\n",
        "\n",
        "        if 1 in keys_list:\n",
        "          position = Dic[1]\n",
        "          position_list = position.split()\n",
        "\n",
        "          Lower = [token.lower() for token in Sentences[i].split()]\n",
        "          Lower[int(position_list[0])] = 'UNK'\n",
        "          Lower[int(position_list[1])] = 'UNK'\n",
        "          Sentences[i] = ' '.join(Lower)\n",
        "\n",
        "    if len(Intersec_Discontinuous) > 0:\n",
        "      w = [Intersec_Discontinuous[j].split() for j in range(len(Intersec_Discontinuous))]\n",
        "      if len(Intersec_Discontinuous) == 1:\n",
        "        if len(w[0]) == 2:\n",
        "          position1 = Processed_Tokens_In_Each_Sentence[i].index(w[0][0])\n",
        "          position2 = Processed_Tokens_In_Each_Sentence[i].index(w[0][1])\n",
        "          w1 = []\n",
        "          w1.append(w[0][0])\n",
        "          w1.append(w[0][1])\n",
        "          if position2-position1 > 1:\n",
        "            Lower = [token.lower() for token in Sentences[i].split()]\n",
        "            Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "        elif len(w[0]) == 3:\n",
        "          position1 = Processed_Tokens_In_Each_Sentence[i].index(w[0][0])\n",
        "          position2 = Processed_Tokens_In_Each_Sentence[i].index(w[0][1])\n",
        "          position3 = Processed_Tokens_In_Each_Sentence[i].index(w[0][2])\n",
        "          w1 = []\n",
        "          w1.append(w[0][0])\n",
        "          w1.append(w[0][1])\n",
        "          w1.append(w[0][2])\n",
        "          if position2-position1 == 1 and position3-position2 > 1:\n",
        "            Lower = [token.lower() for token in Sentences[i].split()]\n",
        "            Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "        else:\n",
        "          position1 = Processed_Tokens_In_Each_Sentence[i].index(w[0][0])\n",
        "          position2 = Processed_Tokens_In_Each_Sentence[i].index(w[0][1])\n",
        "          position3 = Processed_Tokens_In_Each_Sentence[i].index(w[0][2])\n",
        "          position4 = Processed_Tokens_In_Each_Sentence[i].index(w[0][3])\n",
        "          w1 = []\n",
        "          w1.append(w[0][0])\n",
        "          w1.append(w[0][1])\n",
        "          w1.append(w[0][2])\n",
        "          w1.append(w[0][3])\n",
        "          if position2-position1 == 1 and position3-position2 > 1 and position4-position3 > 1:\n",
        "            Lower = [token.lower() for token in Sentences[i].split()]\n",
        "            Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "          if position2-position1 == 1 and position3-position2 > 1 and position4-position3 == 1:\n",
        "            Lower = [token.lower() for token in Sentences[i].split()]\n",
        "            Sentences[i] = ' '.join(['UNK' if token in w1 else token for token in Lower])\n",
        "\n",
        "  Sentences.append('')\n",
        "\n",
        "  context = '.'.join(Sentences)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M4onQ5aRKia"
      },
      "source": [
        "The code above defines a total of 13 ablation methods.\n",
        "\n",
        "The following code describes the process of applying each ablation method on the development set of each MRC dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5HHUU-vzL8s"
      },
      "source": [
        "# create a folder named SQuAD_v1.1 to save the training and development dataset\n",
        "!mkdir '/content/gdrive/My Drive/SQuAD_v1.1'\n",
        "\n",
        "# download the training and development set of SQuAD v1.1 and save them to the SQuAD_v1.1 folder\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json -O '/content/gdrive/My Drive/SQuAD_v1.1/train-v1.1.json'\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -O '/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXpsr30j1XQH"
      },
      "source": [
        "# create a folder named SQuAD_v2.0 to save the training and development dataset\n",
        "!mkdir '/content/gdrive/My Drive/SQuAD_v2.0'\n",
        "\n",
        "# download the training and development set of SQuAD v2.0 and save them to the SQuAD_v2.0 folder\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O '/content/gdrive/My Drive/SQuAD_v2.0/train-v2.0.json'\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O '/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2nR17sm0tI1"
      },
      "source": [
        "!mkdir '/content/gdrive/My Drive/SWAG'\n",
        "\n",
        "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/train.csv -O '/content/gdrive/My Drive/SWAG/train.csv'\n",
        "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/val.csv -O '/content/gdrive/My Drive/SWAG/val.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUJyRKi2g1NO"
      },
      "source": [
        "Apply ablation methods on the SQuAD 1.1 development set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiMUPGl0ypCH"
      },
      "source": [
        "import json\n",
        "import tqdm\n",
        "\n",
        "def load_squad_first_devjson():\n",
        "  squad_first_devjson_file = open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1.json', 'r')\n",
        "  data = json.load(squad_first_devjson_file)\n",
        "  squad_first_devjson_file.close()\n",
        "  return data\n",
        "\n",
        "SQuAD_FVersion = load_squad_first_devjson()\n",
        "# print(SQuAD_FVersion)\n",
        "\n",
        "# run five times\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Random_Shuffle_Sentences_With_Explicit_Connectives(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-Ranshuffle.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_2(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense2.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_2_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense2-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_3(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense3.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_3_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense3-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_4(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense4.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_4_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense4-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_12(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense12.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_12_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense12-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_20(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense20.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_20_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense20-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_24(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense24.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_FVersion['data']))):\n",
        "  list_paragraphs = SQuAD_FVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_24_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1-MECWSense24-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_FVersion, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N61YGrQfsfwq"
      },
      "source": [
        "Apply ablation methods on the SQuAD 2.0 development set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRdKJ7pjsmkQ"
      },
      "source": [
        "import json\n",
        "import tqdm\n",
        "\n",
        "def load_squad_second_devjson():\n",
        "  squad_second_devjson_file = open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0.json', 'r')\n",
        "  data = json.load(squad_second_devjson_file)\n",
        "  squad_second_devjson_file.close()\n",
        "  return data\n",
        "\n",
        "SQuAD_SVersion = load_squad_second_devjson()\n",
        "# print(SQuAD_SVersion)\n",
        "\n",
        "# run five times\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Random_Shuffle_Sentences_With_Explicit_Connectives(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-Ranshuffle.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_2(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense2.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_2_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense2-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_3(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense3.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_3_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense3-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_4(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense4.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_4_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense4-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_12(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense12.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_12_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense12-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_20(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense20.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_20_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense20-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_24(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense24.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(SQuAD_SVersion['data']))):\n",
        "  list_paragraphs = SQuAD_SVersion['data'][article_id]['paragraphs']\n",
        "  for paragraph in list_paragraphs:\n",
        "    context = paragraph['context']\n",
        "    paragraph['context'] = Mask_Explicit_Connectives_With_Sense_24_Full(context)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0-MECWSense24-Full.json', 'w') as f:\n",
        "  json.dump(SQuAD_SVersion, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH9XlxjixAWP"
      },
      "source": [
        "Apply ablation methods on the SWAG development set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QVQTjMxxHEw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_frame = pd.read_csv('/content/gdrive/My Drive/SWAG/val.csv')\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_2(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_2(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense2/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_3(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_3(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense3/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_4(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_4(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense4/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_12(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_12(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense12/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_20(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_20(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense20/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_24(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_24(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense24/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_2_Full(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_2_Full(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense2-Full/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_3_Full(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_3_Full(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense3-Full/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_4_Full(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_4_Full(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense4-Full/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_12_Full(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_12_Full(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense12-Full/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_20_Full(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_20_Full(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense20-Full/val.csv', index=False)\n",
        "\n",
        "data_frame['startphrase'] = data_frame['startphrase'].map(lambda x: Mask_Explicit_Connectives_With_Sense_24_Full(x.split('.')[0]) + x.split('.')[1])\n",
        "data_frame['sent1'] = data_frame['sent1'].map(lambda x: Mask_Explicit_Connectives_With_Sense_24_Full(x))\n",
        "data_frame.to_csv('/content/gdrive/My Drive/SWAG-MECWSense24-Full/val.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xig-T6pBpwQi"
      },
      "source": [
        "Commands used to fine-tune the BERT-base (uncased) model on the training set of each dataset and evaluate it on the modified development set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBj4OsAhp9TD"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75IBPs9hqQwt"
      },
      "source": [
        "# Start training and evaluating on SQuAD 1.1\n",
        "!python '/content/gdrive/My Drive/SQuAD_v1.1/run_squad.py' \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --train_file '/content/gdrive/My Drive/SQuAD_v1.1/train-v1.1.json' \\\n",
        "  --predict_file '/content/gdrive/My Drive/SQuAD_v1.1/dev-v1.1.json' \\ # Change the file path\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 2.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir './bert-base-uncased-squadv1.1/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCIQD_Mzqi7h"
      },
      "source": [
        "# Start training and evaluating on SQuAD 2.0\n",
        "!python '/content/gdrive/My Drive/SQuAD_v2.0/run_squad.py' \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --version_2_with_negative \\\n",
        "  --train_file '/content/gdrive/My Drive/SQuAD_v2.0/train-v2.0.json' \\\n",
        "  --predict_file '/content/gdrive/My Drive/SQuAD_v2.0/dev-v2.0.json' \\ # Change the file path\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 2.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir './bert-base-uncased-squadv2.0/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J2bMlOaq1jA"
      },
      "source": [
        "# Start training and evaluating on SWAG\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdXuPBMIrCe-"
      },
      "source": [
        "!python ./transformers/examples/multiple-choice/run_multiple_choice.py \\\n",
        "  --task_name swag \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --data_dir '/content/gdrive/My Drive/SWAG' \\ # Change the data folder\n",
        "  --learning_rate 5e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --max_seq_length 80 \\\n",
        "  --output_dir './bert-base-uncased-swag/' \\\n",
        "  --per_gpu_eval_batch_size=8 \\\n",
        "  --per_gpu_train_batch_size=8 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "  --overwrite_output"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}